# -*- coding: utf-8 -*-
"""CS547 HW2 Coding Question.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ty0UvD4hgCPG8W9ECTAaQWENGk7Uy0fB
"""

import torch
from torch.nn import functional as F
import matplotlib.pyplot as plt
import numpy as np

X = torch.normal(0, 1, size=(200, 1), generator=torch.manual_seed(0))
Y = torch.ones(size=(200, 1))
Y[X < 0] = 0

class LogisticRegression(torch.nn.Module):
  def __init__(self):
    super(LogisticRegression, self).__init__()
    self.linear = torch.nn.Linear(1, 1)

  def forward(self, x):
    y_pred = torch.sigmoid(self.linear(x))
    return y_pred

"""(1) Carry out logistic regression, and note the width of the transition layer."""

model = LogisticRegression()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)

for epoch in range(20000):
  model.train()
  optimizer.zero_grad()
  y_pred = model(X)
  loss = criterion(y_pred, Y)

  loss.backward()
  optimizer.step()

  # if epoch % 100 == 0:
  #   print(loss.item())

m_opt = model.linear.weight.item()
b_opt = model.linear.bias.item()

fig = plt.figure(figsize=(5, 5), dpi=100)
ax = fig.add_subplot(111)
x = np.linspace(-10, 10, 1000)
z = 1./(1+np.exp(-(m_opt*x + b_opt)))
x_data = X.detach().numpy()
y_data = y_pred.detach().numpy()
ax.scatter(x_data[Y==0], y_data[Y==0], s=5, c='red', label='class = 0')
ax.scatter(x_data[Y==1], y_data[Y==1], s=5, c='blue', label='class = 1')
ax.plot(x, z, color='black', label='1/m={0:.2f}'.format(1./m_opt),
        linewidth=0.5)

ax.set_xlim([-5, 5])
ax.set_xlabel('z')
ax.set_ylabel('Probability')
ax.legend();

"""The width of the transition layer is 0.06.

(2) Flip 5 points on each side of the origin to the 'wrong' label and note the width of the transition layer.
"""

Y5 = torch.clone(Y)
Y5[torch.nonzero(X>0)[:, 0][:5]] = 0 
Y5[torch.nonzero(X<=0)[:, 0][:5]] = 1

model = LogisticRegression()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)

for epoch in range(20000):
  model.train()
  optimizer.zero_grad()
  y_pred = model(X)
  loss = criterion(y_pred, Y5)

  loss.backward()
  optimizer.step()

  # if epoch % 100 == 0:
  #   print(loss.item())

m_opt = model.linear.weight.item()
b_opt = model.linear.bias.item()

fig = plt.figure(figsize=(5, 5), dpi=100)
ax = fig.add_subplot(111)
x = np.linspace(-10, 10, 1000)
z = 1./(1+np.exp(-(m_opt*x + b_opt)))
x_data = X.detach().numpy()
y_data = y_pred.detach().numpy()
ax.scatter(x_data[Y5==0], y_data[Y5==0], s=5, c='red', label='class = 0')
ax.scatter(x_data[Y5==1], y_data[Y5==1], s=5, c='blue', label='class = 1')
ax.plot(x, z, color='black', label='1/m={0:.2f}'.format(1./m_opt),
        linewidth=0.5)

ax.set_xlim([-5, 5])
ax.set_xlabel('z')
ax.set_ylabel('Probability')
ax.legend();

"""The width of the transition layer is increased to 0.21.

(3) Repeat this for 15, 20, 25, 30, 35 points and plot the width of the transition layer as a function of the
number of points with the 'wrong' label.
"""

num_flip_pts = [0, 5, 15, 20, 25, 30, 35]
widths = [0.06, 0.21]
for i in range(5):
  Y_ = torch.clone(Y)
  Y_[torch.nonzero(X>0)[:, 0][:num_flip_pts[i+2]]] = 0 
  Y_[torch.nonzero(X<=0)[:, 0][:num_flip_pts[i+2]]] = 1

  model = LogisticRegression()
  criterion = torch.nn.BCELoss(reduction='mean')
  optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)

  for epoch in range(20000):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X)
    loss = criterion(y_pred, Y_)
    loss.backward()
    optimizer.step()

  m_opt = model.linear.weight.item()
  widths += [1./m_opt]

fig = plt.figure(figsize=(5, 5), dpi=100)
ax = fig.add_subplot(111)

ax.plot(num_flip_pts, widths, '.-', linewidth=0.5, )
print(widths)
ax.set_xlabel('Number of flipped points')
ax.set_ylabel('Widths')

"""The widths of the transition layter increase as more points are labeled wrong.

The widths are: 0.06, 0.21, 0.624, 0.794, 0.97, 1.40, 2.63.
"""

